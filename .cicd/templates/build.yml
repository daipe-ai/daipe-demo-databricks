jobs:
## TEST
- job: Test
  displayName: Test
  dependsOn: []
  continueOnError: false
  pool:
    vmImage: 'ubuntu-20.04'
  workspace:
    clean: all

  steps:
  # set conda-forge repository
  - task: Bash@3
    displayName: 'Set conda-forge repository'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        eval "$(conda shell.bash hook)"
        conda config --remove channels defaults
        conda config --append channels conda-forge
  # environment initialization
  - task: Bash@3
    displayName: 'Python environment initialization'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        cp .env.dist .env
        export SHELL=$SHELL
        chmod +x env-init.sh
        ./env-init.sh -y --verbose
  # check coding standards
  - task: Bash@3
    displayName: 'Testing coding standards'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        eval "$(conda shell.bash hook)"
        conda activate $PWD/.venv
        source $HOME/.poetry/env
        poe flake8
  # run Container tests
  - task: Bash@3
    displayName: 'Running container tests'
    inputs:
      targetType: inline
      script: |
        set -euo pipefail
        eval "$(conda shell.bash hook)"
        conda activate $PWD/.venv
        ~/.poetry/bin/poetry install --no-root --no-dev # remove all dev dependencies
        pip install databricks-connect==9.1.2 # pyspark is still needed
        python src/$(ls src)/ContainerTest.py